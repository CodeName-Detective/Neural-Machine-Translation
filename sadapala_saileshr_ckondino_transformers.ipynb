{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torchtext==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "! pip install -U spacy\n",
    "! python -m spacy download fr_core_news_sm\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(device='mps')\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(device='cuda')\n",
    "else:\n",
    "    DEVICE = torch.device(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(device='cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "spacy_fr = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "french = torchtext.data.Field(tokenize= lambda text: [token.text for token in spacy_en.tokenizer(text)],\n",
    "                              lower = True,\n",
    "                              init_token = '<sos>',\n",
    "                              eos_token = '<eos>')\n",
    "\n",
    "english = torchtext.data.Field(tokenize= lambda text: [token.text for token in spacy_fr.tokenizer(text)],\n",
    "                              lower = True,\n",
    "                              init_token = '<sos>',\n",
    "                              eos_token = '<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {\n",
    "    'English': ('eng', english),\n",
    "    'French': ('fre', french)\n",
    "\n",
    "}\n",
    "\n",
    "train_data, test_data = torchtext.data.TabularDataset.splits(\n",
    "    path='data/',\n",
    "    train='train_25.csv',\n",
    "    test = 'test_25.csv',\n",
    "    format='csv',\n",
    "    fields=fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "english.build_vocab(train_data, max_size = 10000, min_freq = 2)\n",
    "french.build_vocab(train_data, max_size = 10000, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    batch_size = batch_size,\n",
    "    sort_within_batch = True, # Protizes to have examples are of similar length in a batch, because it reduces padding and save compute.\n",
    "    sort_key = lambda x: len(getattr(x, 'eng')), # Protizes to have examples are of similar length in a batch, because it reduces padding and save compute.\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = french.vocab.stoi['<pad>']\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch_train(model: torch.nn.Module, data_loader_train: torch.utils.data.DataLoader,\n",
    "                    loss_criterion: torch.nn, optim_alog: torch.optim) -> tuple:\n",
    "    \"\"\"Function that trains the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Pytorch model we want to train.\n",
    "        data_loader_train (torch.utils.data.DataLoader): Pytorch dataloader that carries training data.\n",
    "        loss_criterion (torch.nn): Pytorch loss criteria on which we calculate loss.\n",
    "        optim_alog (torch.optim): Opimiztion algoritham that we use to update model weights.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple carrying Train loss and accuracy\n",
    "    \"\"\"\n",
    "    batch_loss_train = []\n",
    "    batch_counter = 0\n",
    "    for batch in data_loader_train:\n",
    "        input_text = batch.eng.to(DEVICE)\n",
    "        target_text = batch.fre.to(DEVICE)\n",
    "\n",
    "\n",
    "        # Enabling model training.\n",
    "        model.train(True)\n",
    "\n",
    "\n",
    "        #Setting gradients to zero to prevent gradient accumulation.\n",
    "        optim_alog.zero_grad()\n",
    "\n",
    "        # Forward pass.\n",
    "        y_pred_prob = model(input_text, target_text)\n",
    "\n",
    "        y_pred_prob = y_pred_prob[1:].reshape(-1, y_pred_prob.shape[2])\n",
    "        target_text = target_text[1:].reshape(-1)\n",
    "\n",
    "        loss = loss_criterion(y_pred_prob, target_text)\n",
    "\n",
    "        batch_loss_train.append(loss.item())\n",
    "\n",
    "        # Back Propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Updating weights\n",
    "        optim_alog.step()\n",
    "        \n",
    "        batch_counter += 1\n",
    "\n",
    "        del(input_text)\n",
    "        del(target_text)\n",
    "\n",
    "    return sum(batch_loss_train)/batch_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model: torch.nn.Module, data_loader_val: torch.utils.data.DataLoader, loss_criterion: torch.nn) -> tuple:\n",
    "    \"\"\"Function that calculates test accuracy\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Pytorch model we want to make inference on.\n",
    "        data_loader_val (torch.utils.data.DataLoader): Pytorch dataloader that carries validation data.\n",
    "        loss_criterion (torch.nn): Pytorch loss criteria on which we calculate loss.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple carrying Test loss and accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    batch_loss_train = []\n",
    "    batch_counter = 0\n",
    "\n",
    "    for batch in data_loader_val:\n",
    "        input_text = batch.eng.to(DEVICE)\n",
    "        target_text = batch.fre.to(DEVICE)\n",
    "\n",
    "        # Disabiling model training.\n",
    "        model.train(False)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            # Forward Pass\n",
    "            y_pred_prob = model(input_text, target_text)\n",
    "\n",
    "            y_pred_prob = y_pred_prob[1:].reshape(-1, y_pred_prob.shape[2])\n",
    "            target_text = target_text[1:].reshape(-1)\n",
    "\n",
    "            # Calculating Loss\n",
    "            loss = loss_criterion(y_pred_prob, target_text)\n",
    "            batch_loss_train.append(loss.item())\n",
    "\n",
    "        batch_counter += 1\n",
    "\n",
    "        del(input_text)\n",
    "        del(target_text)\n",
    "\n",
    "    return sum(batch_loss_train)/batch_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model: torch.nn.Module, data_loader_train: torch.utils.data.DataLoader, data_loader_val: torch.utils.data.DataLoader,\n",
    "                  epochs:int, loss_criterion: torch.nn, optim_alog: torch.optim)-> dict:\n",
    "    \"\"\"Function that trains the model for the given number of epochs\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Pytorch model we want to train.\n",
    "        data_loader_train (torch.utils.data.DataLoader): Pytorch dataloader that carries training data.\n",
    "        data_loader_val (torch.utils.data.DataLoader): Pytorch dataloader that carries validation data.\n",
    "        epochs (int): Count of EPOCHS\n",
    "        loss_criterion (torch.nn): Pytorch loss criteria on which we calculate loss.\n",
    "        optim_alog (torch.optim): Opimiztion algoritham that we use to update model weights.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary that carries the output metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "\n",
    "    # Loop that iterates over each EPOCH\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        #Train the model for one EPOCH\n",
    "        epoch_loss = one_epoch_train(model, data_loader_train, loss_criterion, optim_alog)\n",
    "        loss_train.append(epoch_loss)\n",
    "\n",
    "        # Caluclating Testing results\n",
    "        val_loss = inference(model, data_loader_val, loss_criterion)\n",
    "        loss_val.append(val_loss)\n",
    "\n",
    "        if (epoch+1)%1 == 0:\n",
    "            print('For Epoch {} We Train Loss:{}, Val Loss:{}'.format(epoch+1, epoch_loss,val_loss))\n",
    "    return {'training_loss':loss_train, 'val_loss':loss_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(epochs: int,metrics: dict) -> None:\n",
    "    \"\"\"Plot the graphs of Training and Testing Accuracy and Loss across Epoches\n",
    "\n",
    "    Args:\n",
    "        epochs (int): Number of Epochs\n",
    "        metrics (dict): A dictionary containing Test and Training datasets' Loss and accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(list(range(epochs)), metrics['training_loss'])\n",
    "    plt.plot(list(range(epochs)), metrics['val_loss'])\n",
    "    plt.grid()\n",
    "    plt.legend(['Train', 'Test'])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train and Validation loss across epochs')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_translator(model: torch.nn.Module, sentence: str, src_corpus: torchtext.data.Field, tgt_corpus: torchtext.data.Field) -> list:\n",
    "    \"\"\"Given the model and english sentence it will translate the english sentence to french.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Model): Pytorch Model\n",
    "        sentence (str): English sentence\n",
    "        src_corpus (torchtext.data.Field): English Corpus (Source Torchtext data field)\n",
    "        tgt_corpus (torchtext.data.Field): French Corpus (Destination Torchtext data field)\n",
    "\n",
    "    Returns:\n",
    "        list: List of words \n",
    "    \"\"\"\n",
    "    \n",
    "    # Checking If the sentence is string or not.\n",
    "    if isinstance(sentence, str):\n",
    "        tokens = [token.text.lower() for token in spacy_en(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "        \n",
    "    # Attaching <SOS> token at the beginning of the source sentence\n",
    "    tokens.insert(0, src_corpus.init_token)\n",
    "    \n",
    "    # Attaching <EOS> token at the end of the source sentence\n",
    "    tokens.append(src_corpus.eos_token)\n",
    "    \n",
    "    # Converting the soruce text to sentence vector \n",
    "    sentence_vector = [src_corpus.vocab.stoi[token] for token in tokens]\n",
    "    \n",
    "    # Creating a tensor from the vector\n",
    "    sentence_tensor = torch.LongTensor(sentence_vector).unsqueeze(1).to(DEVICE)\n",
    "    \n",
    "    # Performing encoding\n",
    "    with torch.inference_mode():\n",
    "        hidden_state, cell_state = model.encoder(sentence_tensor)\n",
    "        \n",
    "    # Attaching <SOS> token at the beginning of the destination sentence. \n",
    "    outputs = [tgt_corpus.vocab.stoi[\"<sos>\"]]\n",
    "    \n",
    "    # Iteration over and producing the sequnce of words of the translated sentence. \n",
    "    for _ in range(20):\n",
    "        \n",
    "        # Getting previous word to pass it to decoder\n",
    "        previous_word = torch.LongTensor([outputs[-1]]).to(DEVICE)\n",
    "        \n",
    "        # Performing Decoding\n",
    "        with torch.inference_mode():\n",
    "            output, hidden_state, cell_state = model.decoder(previous_word, hidden_state, cell_state)\n",
    "            # Predicting the word\n",
    "            word_pred = torch.argmax(output, axis=1).item()\n",
    "            \n",
    "        # Append the Predicted word \n",
    "        outputs.append(word_pred)\n",
    "        \n",
    "        # On reaching end of the sentence break the loop\n",
    "        if torch.argmax(output, axis=1).item() == tgt_corpus.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "    \n",
    "    # Converting translated sentence vector to translated sentence.\n",
    "    translated_sentence = [tgt_corpus.vocab.itos[idx] for idx in outputs]\n",
    "\n",
    "    return translated_sentence[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_bleu(dataset: torchtext.data.TabularDataset.splits, model: torch.nn.Module, src_corpus: torchtext.data.Field, tgt_corpus: torchtext.data.Field) -> int:\n",
    "    \"\"\" Get BLeU score of the given dataset and its translation.\n",
    "\n",
    "    Args:\n",
    "        dataset (torchtext.data.TabularDataset.splits): torch text data set\n",
    "        model (torch.nn.Model): pytorch model\n",
    "        src_corpus (torchtext.data.Field): English Corpus (Source Torchtext data field)\n",
    "        tgt_corpus (torchtext.data.Field): French Corpus (Destination Torchtext data field)\n",
    "\n",
    "    Returns:\n",
    "        int: Bleu score of the translation.\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    outputs = []\n",
    "\n",
    "    for translation_record in dataset:\n",
    "        source_sentence = vars(translation_record)[\"eng\"]\n",
    "        target_sentence = vars(translation_record)[\"fre\"]\n",
    "\n",
    "        translated_sentence = sentence_translator(model, source_sentence, src_corpus, tgt_corpus)\n",
    "        translated_sentence = translated_sentence[:-1]\n",
    "\n",
    "        targets.append([target_sentence])\n",
    "        outputs.append(translated_sentence)\n",
    "\n",
    "    return torchtext.data.metrics.bleu_score(outputs, targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, embedding_size, src_vocab_size, target_vocab_size, src_pad_idx, num_heads, num_encoder_layers,\n",
    "                 num_decoder_layers, forward_expansion, dropout, max_len):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_word_embedding = torch.nn.Embedding(src_vocab_size, embedding_size)\n",
    "        # Since Transformers are permuationally invariant\n",
    "        self.src_position_embedding = torch.nn.Embedding(max_len, embedding_size)\n",
    "\n",
    "        self.trg_word_embedding = torch.nn.Embedding(target_vocab_size, embedding_size)\n",
    "        self.trg_position_embedding = torch.nn.Embedding(max_len, embedding_size)\n",
    "\n",
    "\n",
    "        self.transformer = torch.nn.Transformer(embedding_size, num_heads, num_encoder_layers, num_decoder_layers,\n",
    "                                                forward_expansion, dropout)\n",
    "        self.linear = torch.nn.Linear(embedding_size, target_vocab_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        # Src shape: (src_len, N) -> By pyrotch implementation takes it as opposit (N, Src_len) hence transpose\n",
    "        src_mask = src.transpose(0,1) == self.src_pad_idx\n",
    "        return src_mask.to(DEVICE)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_seq_length, N = src.shape\n",
    "        trg_seq_length, N = trg.shape\n",
    "        \n",
    "        src_positions = (\n",
    "            torch.arange(0, src_seq_length).unsqueeze(1).expand(src_seq_length, N).to(DEVICE)\n",
    "            )\n",
    "\n",
    "        trg_positions = (\n",
    "            torch.arange(0, trg_seq_length).unsqueeze(1).expand(trg_seq_length, N).to(DEVICE)\n",
    "            )\n",
    "\n",
    "        embed_src = self.dropout(\n",
    "            (self.src_word_embedding(src)+self.src_position_embedding(src_positions))\n",
    "        )\n",
    "\n",
    "        embed_trg = self.dropout(\n",
    "            (self.trg_word_embedding(trg)+self.trg_position_embedding(trg_positions))\n",
    "        )\n",
    "\n",
    "        src_padding_mask = self.make_src_mask(src)\n",
    "        \n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(DEVICE)\n",
    "        \n",
    "        print(src_padding_mask.shape)\n",
    "        print(trg_mask.shape)\n",
    "        \n",
    "        print(embed_src.shape)\n",
    "        print(embed_trg.shape)\n",
    "\n",
    "        out = self.transformer(embed_src, embed_trg, src_padding_mask, trg_mask)\n",
    "        print('ZZZZZ')\n",
    "\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_model = Transformer(\n",
    "    embedding_size = 512,\n",
    "    src_vocab_size= len(english.vocab), \n",
    "    target_vocab_size= len(french.vocab), \n",
    "    src_pad_idx= english.vocab.stoi[\"<pad>\"], \n",
    "    num_heads=8, \n",
    "    num_encoder_layers=3,\n",
    "    num_decoder_layers=3, \n",
    "    forward_expansion=2048, \n",
    "    dropout=0.10, \n",
    "    max_len=25\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "optimizer = torch.optim.Adam(trans_model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 5])\n",
      "torch.Size([9, 9])\n",
      "torch.Size([5, 64, 512])\n",
      "torch.Size([9, 64, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The shape of the 2D attn_mask is torch.Size([64, 5]), but should be (5, 5).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output_metrics \u001b[39m=\u001b[39m training_loop(model \u001b[39m=\u001b[39;49m trans_model, data_loader_train \u001b[39m=\u001b[39;49m train_iterator, data_loader_val \u001b[39m=\u001b[39;49m test_iterator,\n\u001b[1;32m      2\u001b[0m                                epochs \u001b[39m=\u001b[39;49m EPOCHS, loss_criterion \u001b[39m=\u001b[39;49m criterion, optim_alog \u001b[39m=\u001b[39;49m optimizer)\n",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, data_loader_train, data_loader_val, epochs, loss_criterion, optim_alog)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m# Loop that iterates over each EPOCH\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m     \u001b[39m#Train the model for one EPOCH\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     epoch_loss \u001b[39m=\u001b[39m one_epoch_train(model, data_loader_train, loss_criterion, optim_alog)\n\u001b[1;32m     25\u001b[0m     loss_train\u001b[39m.\u001b[39mappend(epoch_loss)\n\u001b[1;32m     27\u001b[0m     \u001b[39m# Caluclating Testing results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 29\u001b[0m, in \u001b[0;36mone_epoch_train\u001b[0;34m(model, data_loader_train, loss_criterion, optim_alog)\u001b[0m\n\u001b[1;32m     26\u001b[0m optim_alog\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     28\u001b[0m \u001b[39m# Forward pass.\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m y_pred_prob \u001b[39m=\u001b[39m model(input_text, target_text)\n\u001b[1;32m     31\u001b[0m y_pred_prob \u001b[39m=\u001b[39m y_pred_prob[\u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, y_pred_prob\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m])\n\u001b[1;32m     32\u001b[0m target_text \u001b[39m=\u001b[39m target_text[\u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/programming_environments/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[42], line 56\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mprint\u001b[39m(embed_src\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     54\u001b[0m \u001b[39mprint\u001b[39m(embed_trg\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 56\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(embed_src, embed_trg, src_padding_mask, trg_mask)\n\u001b[1;32m     57\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mZZZZZ\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(out)\n",
      "File \u001b[0;32m~/Documents/programming_environments/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/programming_environments/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:145\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m src\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39mor\u001b[39;00m tgt\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model:\n\u001b[1;32m    143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 145\u001b[0m memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src, mask\u001b[39m=\u001b[39;49msrc_mask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[1;32m    146\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[39m=\u001b[39mtgt_mask, memory_mask\u001b[39m=\u001b[39mmemory_mask,\n\u001b[1;32m    147\u001b[0m                       tgt_key_padding_mask\u001b[39m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    148\u001b[0m                       memory_key_padding_mask\u001b[39m=\u001b[39mmemory_key_padding_mask)\n\u001b[1;32m    149\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Documents/programming_environments/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/programming_environments/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:315\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_causal \u001b[39m=\u001b[39m make_causal\n\u001b[1;32m    314\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 315\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, is_causal\u001b[39m=\u001b[39;49mis_causal, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    318\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/programming_environments/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/programming_environments/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:591\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    589\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m    590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[39m=\u001b[39;49mis_causal))\n\u001b[1;32m    592\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    594\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/programming_environments/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:599\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[1;32m    598\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 599\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[1;32m    600\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    601\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    602\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, is_causal\u001b[39m=\u001b[39;49mis_causal)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    603\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/Documents/programming_environments/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/programming_environments/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/activation.py:1205\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1192\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1193\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1202\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1203\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1205\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1206\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1207\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1208\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1209\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1210\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1211\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1212\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1213\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1214\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1215\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1217\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/Documents/programming_environments/pytorch_env/lib/python3.11/site-packages/torch/nn/functional.py:5251\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5249\u001b[0m     correct_2d_size \u001b[39m=\u001b[39m (tgt_len, src_len)\n\u001b[1;32m   5250\u001b[0m     \u001b[39mif\u001b[39;00m attn_mask\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m correct_2d_size:\n\u001b[0;32m-> 5251\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe shape of the 2D attn_mask is \u001b[39m\u001b[39m{\u001b[39;00mattn_mask\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, but should be \u001b[39m\u001b[39m{\u001b[39;00mcorrect_2d_size\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5252\u001b[0m     attn_mask \u001b[39m=\u001b[39m attn_mask\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m   5253\u001b[0m \u001b[39melif\u001b[39;00m attn_mask\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The shape of the 2D attn_mask is torch.Size([64, 5]), but should be (5, 5)."
     ]
    }
   ],
   "source": [
    "output_metrics = training_loop(model = trans_model, data_loader_train = train_iterator, data_loader_val = test_iterator,\n",
    "                               epochs = EPOCHS, loss_criterion = criterion, optim_alog = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(EPOCHS, output_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train BLEU Score:{0}'.format(score_bleu(train_data, trans_model, english, french)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test BLEU Score:{0}'.format(score_bleu(test_data, trans_model, english, french)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
