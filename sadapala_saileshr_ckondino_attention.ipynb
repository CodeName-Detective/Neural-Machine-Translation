{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence - To - Sequence with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torchtext==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "! pip install -U spacy\n",
    "! python -m spacy download fr_core_news_sm\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(device='mps')\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(device='cuda')\n",
    "else:\n",
    "    DEVICE = torch.device(device='cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "spacy_fr = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "french = torchtext.data.Field(tokenize= lambda text: [token.text for token in spacy_en.tokenizer(text)],\n",
    "                              lower = True,\n",
    "                              init_token = '<sos>',\n",
    "                              eos_token = '<eos>')\n",
    "\n",
    "english = torchtext.data.Field(tokenize= lambda text: [token.text for token in spacy_fr.tokenizer(text)],\n",
    "                              lower = True,\n",
    "                              init_token = '<sos>',\n",
    "                              eos_token = '<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {\n",
    "    'English': ('eng', english),\n",
    "    'French': ('fre', french)\n",
    "\n",
    "}\n",
    "\n",
    "train_data, test_data = torchtext.data.TabularDataset.splits(\n",
    "    path='data/',\n",
    "    train='train_simple.csv',\n",
    "    test = 'test_simple.csv',\n",
    "    format='csv',\n",
    "    fields=fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "english.build_vocab(train_data, max_size = 10000, min_freq = 2)\n",
    "french.build_vocab(train_data, max_size = 10000, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    batch_size = batch_size,\n",
    "    sort_within_batch = True, # Protizes to have examples are of similar length in a batch, because it reduces padding and save compute.\n",
    "    sort_key = lambda x: len(getattr(x, 'eng')), # Protizes to have examples are of similar length in a batch, because it reduces padding and save compute.\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = french.vocab.stoi['<pad>']\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch_train(model: torch.nn.Module, data_loader_train: torch.utils.data.DataLoader,\n",
    "                    loss_criterion: torch.nn, optim_alog: torch.optim) -> tuple:\n",
    "    \"\"\"Function that trains the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Pytorch model we want to train.\n",
    "        data_loader_train (torch.utils.data.DataLoader): Pytorch dataloader that carries training data.\n",
    "        loss_criterion (torch.nn): Pytorch loss criteria on which we calculate loss.\n",
    "        optim_alog (torch.optim): Opimiztion algoritham that we use to update model weights.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple carrying Train loss and accuracy\n",
    "    \"\"\"\n",
    "    batch_loss_train = []\n",
    "    batch_counter = 0\n",
    "    for batch in data_loader_train:\n",
    "        input_text = batch.eng.to(DEVICE)\n",
    "        target_text = batch.fre.to(DEVICE)\n",
    "\n",
    "\n",
    "        # Enabling model training.\n",
    "        model.train(True)\n",
    "\n",
    "\n",
    "        #Setting gradients to zero to prevent gradient accumulation.\n",
    "        optim_alog.zero_grad()\n",
    "\n",
    "        # Forward pass.\n",
    "        y_pred_prob = model(input_text, target_text)\n",
    "\n",
    "        y_pred_prob = y_pred_prob[1:].reshape(-1, y_pred_prob.shape[2])\n",
    "        target_text = target_text[1:].reshape(-1)\n",
    "\n",
    "        loss = loss_criterion(y_pred_prob, target_text)\n",
    "\n",
    "        batch_loss_train.append(loss.item())\n",
    "\n",
    "        # Back Propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Updating weights\n",
    "        optim_alog.step()\n",
    "        \n",
    "        batch_counter += 1\n",
    "\n",
    "        del(input_text)\n",
    "        del(target_text)\n",
    "\n",
    "    return sum(batch_loss_train)/batch_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model: torch.nn.Module, data_loader_val: torch.utils.data.DataLoader, loss_criterion: torch.nn) -> tuple:\n",
    "    \"\"\"Function that calculates test accuracy\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Pytorch model we want to make inference on.\n",
    "        data_loader_val (torch.utils.data.DataLoader): Pytorch dataloader that carries validation data.\n",
    "        loss_criterion (torch.nn): Pytorch loss criteria on which we calculate loss.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple carrying Test loss and accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    batch_loss_train = []\n",
    "    batch_counter = 0\n",
    "\n",
    "    for batch in data_loader_val:\n",
    "        input_text = batch.eng.to(DEVICE)\n",
    "        target_text = batch.fre.to(DEVICE)\n",
    "\n",
    "        # Disabiling model training.\n",
    "        model.train(False)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            # Forward Pass\n",
    "            y_pred_prob = model(input_text, target_text)\n",
    "\n",
    "            y_pred_prob = y_pred_prob[1:].reshape(-1, y_pred_prob.shape[2])\n",
    "            target_text = target_text[1:].reshape(-1)\n",
    "\n",
    "            # Calculating Loss\n",
    "            loss = loss_criterion(y_pred_prob, target_text)\n",
    "            batch_loss_train.append(loss.item())\n",
    "\n",
    "        batch_counter += 1\n",
    "\n",
    "        del(input_text)\n",
    "        del(target_text)\n",
    "\n",
    "    return sum(batch_loss_train)/batch_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model: torch.nn.Module, data_loader_train: torch.utils.data.DataLoader, data_loader_val: torch.utils.data.DataLoader,\n",
    "                  epochs:int, loss_criterion: torch.nn, optim_alog: torch.optim)-> dict:\n",
    "    \"\"\"Function that trains the model for the given number of epochs\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Pytorch model we want to train.\n",
    "        data_loader_train (torch.utils.data.DataLoader): Pytorch dataloader that carries training data.\n",
    "        data_loader_val (torch.utils.data.DataLoader): Pytorch dataloader that carries validation data.\n",
    "        epochs (int): Count of EPOCHS\n",
    "        loss_criterion (torch.nn): Pytorch loss criteria on which we calculate loss.\n",
    "        optim_alog (torch.optim): Opimiztion algoritham that we use to update model weights.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary that carries the output metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "\n",
    "    # Loop that iterates over each EPOCH\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        #Train the model for one EPOCH\n",
    "        epoch_loss = one_epoch_train(model, data_loader_train, loss_criterion, optim_alog)\n",
    "        loss_train.append(epoch_loss)\n",
    "\n",
    "        # Caluclating Testing results\n",
    "        val_loss = inference(model, data_loader_val, loss_criterion)\n",
    "        loss_val.append(val_loss)\n",
    "\n",
    "        if (epoch+1)%1 == 0:\n",
    "            print('For Epoch {} We Train Loss:{}, Val Loss:{}'.format(epoch+1, epoch_loss,val_loss))\n",
    "    return {'training_loss':loss_train, 'val_loss':loss_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(epochs: int,metrics: dict) -> None:\n",
    "    \"\"\"Plot the graphs of Training and Testing Accuracy and Loss across Epoches\n",
    "\n",
    "    Args:\n",
    "        epochs (int): Number of Epochs\n",
    "        metrics (dict): A dictionary containing Test and Training datasets' Loss and accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(list(range(epochs)), metrics['training_loss'])\n",
    "    plt.plot(list(range(epochs)), metrics['val_loss'])\n",
    "    plt.grid()\n",
    "    plt.legend(['Train', 'Test'])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train and Validation loss across epochs')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_translator(model: torch.nn.Module, sentence: str, src_corpus: torchtext.data.Field, tgt_corpus: torchtext.data.Field) -> list:\n",
    "    \"\"\"Given the model and english sentence it will translate the english sentence to french.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Model): Pytorch Model\n",
    "        sentence (str): English sentence\n",
    "        src_corpus (torchtext.data.Field): English Corpus (Source Torchtext data field)\n",
    "        tgt_corpus (torchtext.data.Field): French Corpus (Destination Torchtext data field)\n",
    "\n",
    "    Returns:\n",
    "        list: List of words \n",
    "    \"\"\"\n",
    "    \n",
    "    # Checking If the sentence is string or not.\n",
    "    if isinstance(sentence, str):\n",
    "        tokens = [token.text.lower() for token in spacy_en(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "        \n",
    "    # Attaching <SOS> token at the beginning of the source sentence\n",
    "    tokens.insert(0, src_corpus.init_token)\n",
    "    \n",
    "    # Attaching <EOS> token at the end of the source sentence\n",
    "    tokens.append(src_corpus.eos_token)\n",
    "    \n",
    "    # Converting the soruce text to sentence vector \n",
    "    sentence_vector = [src_corpus.vocab.stoi[token] for token in tokens]\n",
    "    \n",
    "    # Creating a tensor from the vector\n",
    "    sentence_tensor = torch.LongTensor(sentence_vector).unsqueeze(1).to(DEVICE)\n",
    "    \n",
    "    # Performing encoding\n",
    "    with torch.inference_mode():\n",
    "        enc_output , hidden_state, cell_state = model.encoder(sentence_tensor)\n",
    "        \n",
    "    # Attaching <SOS> token at the beginning of the destination sentence. \n",
    "    outputs = [tgt_corpus.vocab.stoi[\"<sos>\"]]\n",
    "    \n",
    "    # Iteration over and producing the sequnce of words of the translated sentence. \n",
    "    for _ in range(30):\n",
    "        \n",
    "        # Getting previous word to pass it to decoder\n",
    "        previous_word = torch.LongTensor([outputs[-1]]).to(DEVICE)\n",
    "        \n",
    "        # Performing Decoding\n",
    "        with torch.inference_mode():\n",
    "            context_vector = model.attention(enc_output, hidden_state)\n",
    "            output, hidden_state, cell_state = model.decoder(previous_word, context_vector, hidden_state, cell_state)\n",
    "            # Predicting the word\n",
    "            word_pred = torch.argmax(output, axis=1).item()\n",
    "            \n",
    "        # Append the Predicted word \n",
    "        outputs.append(word_pred)\n",
    "        \n",
    "        # On reaching end of the sentence break the loop\n",
    "        if torch.argmax(output, axis=1).item() == tgt_corpus.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "    \n",
    "    # Converting translated sentence vector to translated sentence.\n",
    "    translated_sentence = [tgt_corpus.vocab.itos[idx] for idx in outputs]\n",
    "\n",
    "    return translated_sentence[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_bleu(dataset: torchtext.data.TabularDataset.splits, model: torch.nn.Module, src_corpus: torchtext.data.Field, tgt_corpus: torchtext.data.Field) -> int:\n",
    "    \"\"\" Get BLeU score of the given dataset and its translation.\n",
    "\n",
    "    Args:\n",
    "        dataset (torchtext.data.TabularDataset.splits): torch text data set\n",
    "        model (torch.nn.Model): pytorch model\n",
    "        src_corpus (torchtext.data.Field): English Corpus (Source Torchtext data field)\n",
    "        tgt_corpus (torchtext.data.Field): French Corpus (Destination Torchtext data field)\n",
    "\n",
    "    Returns:\n",
    "        int: Bleu score of the translation.\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    outputs = []\n",
    "\n",
    "    for translation_record in dataset:\n",
    "        source_sentence = vars(translation_record)[\"eng\"]\n",
    "        target_sentence = vars(translation_record)[\"fre\"]\n",
    "\n",
    "        translated_sentence = sentence_translator(model, source_sentence, src_corpus, tgt_corpus)\n",
    "        translated_sentence = translated_sentence[:-1]\n",
    "\n",
    "        targets.append([target_sentence])\n",
    "        outputs.append(translated_sentence)\n",
    "\n",
    "    return torchtext.data.metrics.bleu_score(outputs, targets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, word_embedding_size, output_size, num_layers, droput_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(droput_prob)\n",
    "        self.embedding = torch.nn.Embedding(input_size, word_embedding_size)\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(word_embedding_size, output_size, num_layers, bidirectional = True)\n",
    "\n",
    "        self.linear_hidden = torch.nn.Linear(output_size*2, output_size)\n",
    "        self.linear_cell = torch.nn.Linear(output_size*2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        word_embedding = self.dropout(self.embedding(x))\n",
    "\n",
    "        encoder_output , (hidden_state, cell_state) = self.lstm(word_embedding)\n",
    "\n",
    "        hidden_state = self.linear_hidden(torch.cat((hidden_state[0:1], hidden_state[1:2]), dim=2))\n",
    "        cell_state = self.linear_cell(torch.cat((cell_state[0:1], cell_state[1:2]), dim=2))\n",
    "\n",
    "        return encoder_output, hidden_state, cell_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        self.energy = torch.nn.Linear(hidden_size*3, 1)\n",
    "        self.softmax = torch.nn.Softmax(dim=0)\n",
    "    \n",
    "    def forward(self, encoder_output, hidden_state):\n",
    "        \n",
    "        seq_len = encoder_output.shape[0]\n",
    "        \n",
    "        h_reshape = hidden_state.repeat(seq_len, 1,1)\n",
    "\n",
    "        energy = torch.nn.functional.relu(self.energy(torch.cat((h_reshape, encoder_output), dim=2)))\n",
    "        \n",
    "        attention = self.softmax(energy)\n",
    "        \n",
    "        attention = attention.permute(1,2,0)\n",
    "\n",
    "        encoder_output = encoder_output.permute(1,0,2)\n",
    "\n",
    "        context_vector = torch.bmm(attention, encoder_output).permute(1,0,2)\n",
    "        \n",
    "        return context_vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, droput_prob):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.droput = torch.nn.Dropout(droput_prob)\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "        self.lstm = torch.nn.LSTM((hidden_size*2)+embedding_size, hidden_size, num_layers)\n",
    "\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, context_vector, hidden_state, cell_state):\n",
    "        # X shape = (batch_size) -> (1,batch_size) because we are predicting one word at a time\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        word_embedding = self.droput(self.embedding(x))\n",
    "\n",
    "        # Embedding shape: (1,batch_size,embedding_size)\n",
    "\n",
    "        rnn_input = torch.cat((context_vector, word_embedding), dim=2)\n",
    "\n",
    "        output, (hidden_state, cell_state) = self.lstm(rnn_input, (hidden_state, cell_state))\n",
    "        # Shape of output: (1,batch_size, hidden_size)\n",
    "\n",
    "        pred = self.linear(output)\n",
    "        #Shape of pred: (1, batch_size, length_of_vocabulary)\n",
    "        pred = pred.squeeze(0)\n",
    "\n",
    "        return pred, hidden_state, cell_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq-2-Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(torch.nn.Module):\n",
    "    def __init__(self, input_size_encoder, encoder_embedding_size, input_size_decoder, decoder_embedding_size, hidden_size, \n",
    "                 num_layers, output_size, encoder_dropout, decoder_dropout):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, encoder_dropout).to(DEVICE)\n",
    "        self.attention = SelfAttention(hidden_size).to(DEVICE)\n",
    "        self.decoder = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, output_size, num_layers, decoder_dropout).to(DEVICE)\n",
    "\n",
    "    def forward(self, src_text, target_text, teacher_force_ratio=0.5):\n",
    "        batch_size = src_text.shape[1]\n",
    "        # Src_text = (target_len, batch_size)\n",
    "        target_len = target_text.shape[0]\n",
    "        target_vocab_size = len(french.vocab)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device=DEVICE)\n",
    "\n",
    "        encoder_ouput, hidden_state, cell_state = self.encoder(src_text)\n",
    "        \n",
    "        # Probability Mass Function defined for the word_selection we should choose\n",
    "        next_word_selection_actions = ['Guess', 'Truth']\n",
    "        next_word_selection_probability_distribution = [teacher_force_ratio, 1-teacher_force_ratio]\n",
    "\n",
    "        # Grab Start Token\n",
    "        x = target_text[0]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            context_vector = self.attention(encoder_ouput, hidden_state)\n",
    "            \n",
    "            output, hidden_state, cell_state = self.decoder(x, context_vector, hidden_state, cell_state)\n",
    "            \n",
    "            outputs[t] = output\n",
    "            # outputs size = (batch_size, target_vocab_size)\n",
    "            best_guess = output.argmax(1)\n",
    "            \n",
    "            # Sampling type word to select based on the above defined Probability Mass Function.\n",
    "            word_to_choose = np.random.choice(a = next_word_selection_actions, size=1, \n",
    "                                              p=next_word_selection_probability_distribution)\n",
    "            \n",
    "            if word_to_choose == 'Guess':\n",
    "                x = best_guess\n",
    "            else:\n",
    "                x = target_text[t]\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_2_seq_model = Seq2Seq(input_size_encoder=len(english.vocab), encoder_embedding_size = 300, \n",
    "                          input_size_decoder = len(french.vocab), decoder_embedding_size = 300, hidden_size = 1024, \n",
    "                          num_layers = 1, output_size = len(french.vocab), encoder_dropout = 0.5, decoder_dropout = 0.5).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "optimizer = torch.optim.Adam(seq_2_seq_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Epoch 1 We Train Loss:3.8611103592404894, Val Loss:3.1014477014541626\n",
      "For Epoch 2 We Train Loss:2.930603009120674, Val Loss:2.6258919707366397\n",
      "For Epoch 3 We Train Loss:2.423686368450238, Val Loss:2.286892737661089\n",
      "For Epoch 4 We Train Loss:2.025315950630577, Val Loss:2.0979833092008318\n",
      "For Epoch 5 We Train Loss:1.6907091064817588, Val Loss:1.9435244415487563\n",
      "For Epoch 6 We Train Loss:1.4304017663761308, Val Loss:1.814642654997962\n",
      "For Epoch 7 We Train Loss:1.2231441219900823, Val Loss:1.7907284668513708\n",
      "For Epoch 8 We Train Loss:1.051338199217608, Val Loss:1.7880278655460902\n",
      "For Epoch 9 We Train Loss:0.9277584321179967, Val Loss:1.7456830867699213\n",
      "For Epoch 10 We Train Loss:0.8260775273013267, Val Loss:1.70395308307239\n",
      "For Epoch 11 We Train Loss:0.7325135465640171, Val Loss:1.7555121779441833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output_metrics \u001b[39m=\u001b[39m training_loop(model \u001b[39m=\u001b[39;49m seq_2_seq_model, data_loader_train \u001b[39m=\u001b[39;49m train_iterator, data_loader_val \u001b[39m=\u001b[39;49m test_iterator,\n\u001b[1;32m      2\u001b[0m                                epochs \u001b[39m=\u001b[39;49m EPOCHS, loss_criterion \u001b[39m=\u001b[39;49m criterion, optim_alog \u001b[39m=\u001b[39;49m optimizer)\n",
      "Cell \u001b[0;32mIn[22], line 24\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, data_loader_train, data_loader_val, epochs, loss_criterion, optim_alog)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m# Loop that iterates over each EPOCH\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m     \u001b[39m#Train the model for one EPOCH\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     epoch_loss \u001b[39m=\u001b[39m one_epoch_train(model, data_loader_train, loss_criterion, optim_alog)\n\u001b[1;32m     25\u001b[0m     loss_train\u001b[39m.\u001b[39mappend(epoch_loss)\n\u001b[1;32m     27\u001b[0m     \u001b[39m# Caluclating Testing results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 45\u001b[0m, in \u001b[0;36mone_epoch_train\u001b[0;34m(model, data_loader_train, loss_criterion, optim_alog)\u001b[0m\n\u001b[1;32m     42\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), max_norm\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[39m# Updating weights\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m optim_alog\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     47\u001b[0m batch_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[39mdel\u001b[39;00m(input_text)\n",
      "File \u001b[0;32m~/Documents/programming_environments/pytorch_env/lib/python3.11/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/programming_environments/pytorch_env/lib/python3.11/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/Documents/programming_environments/pytorch_env/lib/python3.11/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[1;32m    144\u001b[0m         exp_avgs,\n\u001b[1;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    147\u001b[0m         state_steps,\n\u001b[1;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Documents/programming_environments/pytorch_env/lib/python3.11/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m func(params,\n\u001b[1;32m    282\u001b[0m      grads,\n\u001b[1;32m    283\u001b[0m      exp_avgs,\n\u001b[1;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    286\u001b[0m      state_steps,\n\u001b[1;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/Documents/programming_environments/pytorch_env/lib/python3.11/site-packages/torch/optim/adam.py:393\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m--> 393\u001b[0m param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_metrics = training_loop(model = seq_2_seq_model, data_loader_train = train_iterator, data_loader_val = test_iterator,\n",
    "                               epochs = EPOCHS, loss_criterion = criterion, optim_alog = optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(EPOCHS, output_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train BLEU Score:{0}'.format(score_bleu(train_data, seq_2_seq_model, english, french)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU Score:0.23895527154769172\n"
     ]
    }
   ],
   "source": [
    "print('Test BLEU Score:{0}'.format(score_bleu(test_data, seq_2_seq_model, english, french)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
